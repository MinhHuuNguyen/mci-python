{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LESSON 6: K-MEANS CLUSTERING - DBSCAN\n",
    "<table><tr>\n",
    "<td> <img src=\"../images/clustering_logo.png\" width=\"500px\"/> </td>\n",
    "</tr></table>\n",
    "\n",
    "*K-means clustering part of this lecture was refered by [machinelearningcoban.com](https://machinelearningcoban.com/2017/01/01/kmeans/)*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. K-means clustering introduction\n",
    "\n",
    "<img src=\"../images/kmeans_example.png\" width=\"400px\"/>\n",
    "\n",
    "With the dataset contains lots of data points without label, we have to group these data points into K clusters and we expect that samples in the same cluster will have similar features. The algorithm to solve this problem is called ***K-MEANS CLUSTERING***.\n",
    "\n",
    "<img src=\"../images/kmeans_example_5_clusters.png\" width=\"400px\"/>\n",
    "\n",
    "In 2D space, we can see that the region of each cluster is a polygon with a line border. This line border is the mid perpendicular of a line connect 2 centers.\n",
    "\n",
    "Assume that we have N data point $X = [x_1, x_2, \\dots, x_N]$ and K < N is the number of clusters. We have to find $M = [m_1, m_2, \\dots, m_K]$ which is the center (or representatives) of K clusters and $Y = [y_1, y_2, \\dots, y_N]$ which is the label of N data points.\n",
    "\n",
    "Label $y_i$ of a sample is encoded into one-hot type, which means $y_i = [y_{i1}, y_{i2}, \\dots, y_{iK}]$ and $y_{ij} = 1$ if $x_i$ is predicted to belong to cluster $j$.\n",
    "\n",
    "So, we have\n",
    "\n",
    "<center>\n",
    "\\[\n",
    " y_{ik} \\in \\{0, 1\\} \\\\\n",
    " \\sum_{k = 1}^K y_{ik} = 1\n",
    "\\]\n",
    "</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Loss function and Optimizer for K-means clustering\n",
    "The main target of clustering technique is to minimize the distance between each data point and its center.\n",
    "\n",
    "If data point $x_i$ belong to cluster $m_k$, so $y_{ik} = 1$ and $y_{ij} = 0$ with $\\forall j \\neq k$\n",
    "\n",
    "<center>\n",
    "    \\[\n",
    "    D(x_i, m_k) = (x_i - m_k)^2 \\\\\n",
    "    D(x_i, m_k) = \\sum_{j=1}^K y_{ij}(x_i - m_j)^2\n",
    "    \\]\n",
    "</center>\n",
    "\n",
    "We have the loss function for the whole dataset\n",
    "\n",
    "<center>\n",
    "    \\[\n",
    "    \\mathcal{L}(Y, M) = MSE(Y, M)\n",
    "    = \\sum_{i=1}^{N}\\sum_{j=1}^{K}y_{ij}(x_i - m_j)^2\n",
    "    \\]\n",
    "</center>\n",
    "\n",
    "With two variables $Y$ and $M$, to optimize the loss function, we fix one variable and optimize another and vice versa. Specifically, we solve two problems: Fixed $M$, optimize $Y$ and Fix $Y$, optimize $M$ respectively.\n",
    "\n",
    "### Fixed M, optimize Y\n",
    "For each data point $x_i$,\n",
    "\n",
    "<center>\n",
    "    \\[\n",
    "    y_i = arg \\min_{y_i} \\mathcal{L}(y_i) \\\\\n",
    "    = arg \\min_{y_i} \\sum_{j=1}^{K}y_{ij}(x_i - m_j)^2\n",
    "    \\]\n",
    "</center>\n",
    "\n",
    "We need to find only one $y_i$ for each $x_i$ so this problem is solved by assign the label of each $x_i$ as the **nearest center** of it.\n",
    "\n",
    "### Fixed Y, optimize M\n",
    "For each existing cluster,\n",
    "\n",
    "<center>\n",
    "    \\[\n",
    "    m_j = arg \\min_{m_j} \\mathcal{L}(m_j) \\\\\n",
    "    = arg \\min_{m_j} \\sum_{i=1}^{N}y_{ij}(x_i - m_j)^2\n",
    "    \\]\n",
    "</center>\n",
    "\n",
    "We need to find only one $m_j$ for each existing cluster\n",
    "\n",
    "Calculate derivative of $\\mathcal{L}(m_j)$ with $m_j$ and solve the derivative function\n",
    "\n",
    "<center>\n",
    "    \\[\n",
    "    \\frac{\\partial \\mathcal{L}(m_j)}{\\partial m_j} = 2 \\sum_{i=1}^{N}y_{ij}(m_j - x_i) = 0 \\\\\n",
    "    m_j \\sum_{i=1}^{N}y_{ij} = \\sum_{i=1}^{N}y_{ij}x_i \\\\\n",
    "    m_j = \\frac{\\sum_{i=1}^{N}y_{ij}x_i}{\\sum_{i=1}^{N}y_{ij}} \\\\\n",
    "    m_j = \\frac{\\text{Sum of all data points in cluster} j}{\\text{Number of data points in cluster} j}\n",
    "    \\]\n",
    "</center>\n",
    "\n",
    "That's why we call this algorithm ***k-means clustering***.\n",
    "\n",
    "### Overall algorithm\n",
    "***Input:*** Dataset contains N samples, K clusters. <br>\n",
    "***Output:*** N label y for each data sample, K center m for each cluster.\n",
    "\n",
    "***Step 1:*** Randomly choose K data points as initialized cluster center. <br>\n",
    "***Step 2:*** Assign label for each data point by nearest center. <br>\n",
    "***Step 3:*** If the results of ***Step 2*** is same as the previous iteration, stop the algorithm, else continue the next step. <br>\n",
    "***Step 4:*** Calculate the new cluster center by the mean of data point in this cluster. <br>\n",
    "***Step 5:*** Go back to ***Step 2***.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Weaknesses of K-means clustering\n",
    "### We need to define number of clusters K\n",
    "In some cases, we don't know the number of cluster and this is an obstacle while using K-means clustering\n",
    "\n",
    "### Clustering results is highly depended on initialization\n",
    "#### Slow convergence\n",
    "\n",
    "<img src=\"../images/kmeans_slow_converge.gif\" width=\"500px\"/>\n",
    "\n",
    "#### Bad results\n",
    "<img src=\"../images/kmeans_bad_result.gif\" width=\"500px\"/>\n",
    "\n",
    "We can overcome this problem by running K-means clustering multiple times and choose the best results. Or there are some upgraded versions of K-means clustering like K-means++.\n",
    "\n",
    "### Clusters must be round in shape\n",
    "\n",
    "<img src=\"../images/kmeans_diff_cov.gif\" width=\"500px\"/>\n",
    "\n",
    "\n",
    "### K-means clustering doesn't work with non-convex dataset\n",
    "\n",
    "<img src=\"../images/kmeans_smile_face.png\" width=\"500px\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. DBSCAN introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Loss function and Optimizer for DBSCAN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Implementation example\n",
    "### 6.1. Implement from scratch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.2. Use `sklearn`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Homework"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  },
  "notify_time": "5",
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": false,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Table of Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
