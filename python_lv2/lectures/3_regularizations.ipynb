{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LESSON 3: REGULARIZATIONS\n",
    "\n",
    "<img src=\"../images/linear_logistic_regression_logo.jpeg\" width=\"600px\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Regularization introduction\n",
    "\n",
    "One simple way to reduce overfitting model is adding a term into loss function\n",
    "\n",
    "<center>\n",
    "    $J_{\\text{reg}}(w) = J(w) + \\lambda R(w)$\n",
    "</center>\n",
    "\n",
    "the $J_{\\text{reg}}(w)$ is called regularized loss function.\n",
    "\n",
    "Minimizing the $J_{\\text{reg}}(w)$ is minimizing both $J(w)$ term and $R(w)$ term.\n",
    "\n",
    "While minimizing $J(w)$ is the main objective, minimizing $R(w)$ will make some weights in our model become near to zero or, in other words, deactivate these weights.\n",
    "\n",
    "Deactivating some weights will degrade the compexity of our model and it help us reduce overfitting.\n",
    "\n",
    "Sometimes, we call \"deactivating some weights\" as **weights decay**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Popular regularization function\n",
    "### 2.1. Overall formula of Norm\n",
    "\n",
    "**norm p** of $x$ is calculated by\n",
    "\n",
    "<center>\n",
    "    $||x||_p = (|x_1|^p + |x_2|^p + \\dots |x_n|^p)^{\\frac{1}{p}}$\n",
    "</center>\n",
    "\n",
    "### 2.2. L1 regularization\n",
    "\n",
    "<center>\n",
    "    $R(w) = ||w||_1 = |w_1| + |w_2| + \\dots |w_n|$\n",
    "</center>\n",
    "\n",
    "$||w||_1$ is called **norm 1**\n",
    "\n",
    "#### Lasso regression\n",
    "\n",
    "In Linear regression, we can add L1 regularization into the loss function to create ***Lasso regression***.\n",
    "\n",
    "Original MSE for Linear regression,\n",
    "\n",
    "<center>\n",
    "    \\[\n",
    "    MSE(w) \\\\\n",
    "    = \\frac{1}{N}\\sum_{i=1}^{N}(\\hat{y}^{(i)} - y^{(i)})^2 \\\\\n",
    "    = \\frac{1}{N}||\\hat{y} - y||^2_2\n",
    "    \\]\n",
    "</center>\n",
    "\n",
    "MSE for Lasso regression,\n",
    "\n",
    "<center>\n",
    "    \\[\n",
    "    MSE(w)\n",
    "    = \\frac{1}{N}||\\hat{y} - y||^2_2\n",
    "    + ||w||_1\n",
    "    \\]\n",
    "</center>\n",
    "\n",
    "### 2.3. L2 regularization\n",
    "\n",
    "<center>\n",
    "    $R(w) = ||w||^2_2 = w_1^2 + w_2^2 + \\dots w_n^2$\n",
    "</center>\n",
    "\n",
    "$||w||_2$ is called **norm 2**\n",
    "\n",
    "#### Ridge regression\n",
    "\n",
    "In Linear regression, we can add L2 regularization into the loss function to create ***Ridge regression***.\n",
    "\n",
    "MSE for Ridge regression,\n",
    "\n",
    "<center>\n",
    "    \\[\n",
    "    MSE(w)\n",
    "    = \\frac{1}{N}||\\hat{y} - y||^2_2\n",
    "    + ||w||^2_2\n",
    "    \\]\n",
    "</center>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  },
  "notify_time": "5",
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": false,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Table of Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
