{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LESSON 3: LOGISTIC REGRESSION - SOFTMAX REGRESSION\n",
    "<table><tr>\n",
    "<td> <img src=\"../images/linear_logistic_regression_logo.jpeg\" width=\"600px\"/> </td>\n",
    "</tr></table>\n",
    "\n",
    "*This lecture was refered by [machinelearningcoban.com](https://machinelearningcoban.com/2017/01/27/logisticregression/)*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Logistic regression introduction\n",
    "\n",
    "<img src=\"../images/logistic_regression_example.png\" width=\"400px\"/>\n",
    "\n",
    "The prediction of linear regression is calculated by $\\hat{y} = X{\\theta}$. So the output of linear regression is a continuous value ranged from $- \\infty$ to $+ \\infty$.\n",
    "\n",
    "Linear regression is modified to become **LOGISTIC REGRESSION** by apply a logistic activation function to the output of linear regression.\n",
    "\n",
    "<center>\n",
    "    $\\hat{y} = \\sigma(X{\\theta})$\n",
    "</center>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Logistic activation function\n",
    "Sigmoid, Softmax and Tanh are logistic activation functions for classification problem.\n",
    "\n",
    "\n",
    "### 2.1. Sigmoid\n",
    "\n",
    "Sigmoid function get any input value and give an output in range from 0 to 1.\n",
    "\n",
    "This value can be understand as probability.\n",
    "\n",
    "<img src=\"../images/logistic_regression_sigmoid.png\" width=\"300px\"/>\n",
    "\n",
    "### 2.2. Tanh\n",
    "\n",
    "The difference between Sigmoid and Tanh is range of output value.\n",
    "\n",
    "Tanh give an output in range from -1 to 1.\n",
    "\n",
    "<img src=\"../images/logistic_regression_tanh.png\" width=\"300px\"/>\n",
    "\n",
    "We can easily change the range of output value of tanh from $[-1, 1]$ to $[0, 1]$.\n",
    "\n",
    "<center>\n",
    "    \\[\n",
    "    \\text{tanh}(z) = 2\\sigma(2z) - 1\n",
    "    \\]\n",
    "</center>\n",
    "\n",
    "### 2.3. Softmax\n",
    "\n",
    "Like Sigmoid, Softmax give an output in range from 0 to 1.\n",
    "\n",
    "But, while Sigmoid is applied for each value independently, Softmax is applied for all the outputs.\n",
    "\n",
    "<img src=\"../images/logistic_regression_softmax.jpeg\" width=\"300px\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Loss function and Optimizer for Logistic Regression\n",
    "We use sigmoid as logistic activation function for logistic regression.\n",
    "\n",
    "With a sample ${x}^{i}$ in the dataset, we have $\\sigma({x}^{i}{\\theta})$ is a probability of sample ${x}^{i}$ belong to class 1 and $1 - \\sigma({x}^{i}{\\theta})$ is a probability of sample ${x}^{i}$ belong to class 0.\n",
    "\n",
    "<center>\n",
    "    \\[\n",
    "    P(y^i = 1 | x^i, \\theta) = \\sigma(\\theta x^i) = \\hat{y}^i \\\\\n",
    "    P(y^i = 0 | x^i, \\theta) = 1 - \\sigma(\\theta x^i) = 1 - \\hat{y}^i\n",
    "    \\]\n",
    "</center>\n",
    "\n",
    "Combine two above functions, we have a new function to maximize\n",
    "\n",
    "<center>\n",
    "    \\[\n",
    "    P(y^i| x^i, \\theta) = (\\hat{y}^i)^{y^i} (1 - \\hat{y}^i)^{(1 - y^i)} \\\\\n",
    "    P(y|X, \\theta) = \\prod_{i=1}^m P(y^i| x^i, \\theta) = \\prod_{i=1}^m (\\hat{y}^i)^{y^i} (1 - \\hat{y}^i)^{(1 - y^i)}\\\\\n",
    "    P(y|X, \\theta) = (\\hat{y})^{y} (1 - \\hat{y})^{(1 - y)}\n",
    "    \\]\n",
    "</center>\n",
    "\n",
    "We need to find the ${\\theta}$ to maximize the value of above function and this ${\\theta}$ called an ***optimal point***.\n",
    "\n",
    "<center>\n",
    "    ${\\theta}^{*} = \\arg\\max_{\\theta} P(y|X, \\theta)$\n",
    "</center>\n",
    "\n",
    "Every value $\\hat{y}^i$ or $1 - \\hat{y}^i$ is lower than 1.\n",
    "\n",
    "Product of lots of these value can cause numerical error, because the product is almost equal to 0.\n",
    "\n",
    "So we use logarit to change from product to sum, and add the negative sign to build a new loss function - ***CROSS ENTROPY LOSS FUNCTION***.\n",
    "\n",
    "In logistic regression, we have 2 classes so we call it ***BINARY CROSS ENTROPY***.\n",
    "\n",
    "<center>\n",
    "    \\[\n",
    "    \\mathcal{L}(\\theta) = - \\log (P(y|X, \\theta)) \\\\\n",
    "    = - (\\log \\hat{y}^{y} + \\log(1 - \\hat{y})^{(1 - y)}) \\\\\n",
    "    = - (y \\log \\hat{y} + (1 - y) \\log(1 - \\hat{y})) \\\\\n",
    "    = - \\sum_i^m (y^i \\log \\hat{y}^i + (1 - y^i) \\log(1 - \\hat{y}^i))\n",
    "    \\]\n",
    "</center>\n",
    "\n",
    "Instead of finding ${\\theta}^{*} = \\arg\\max_{\\theta} P(y|X, \\theta)$, we find ${\\theta}^{*} = \\arg\\min_{\\theta} - \\log (P(y|X, \\theta))$\n",
    "\n",
    "To find the optimal point $\\vec{\\theta}^{*}$, we solve the equation:\n",
    "\n",
    "<center>\n",
    "    \\[\n",
    "    \\frac{\\partial \\mathcal{L}}{\\partial {\\theta}}\n",
    "    = \\frac{\\partial \\mathcal{L}}{\\partial {\\sigma}} \\cdot \\frac{\\partial \\sigma}{\\partial {\\theta}}\n",
    "    = \\frac{\\hat{y} - y}{\\hat{y}(1 - \\hat{y})} \\cdot \\frac{\\partial \\sigma}{\\partial {\\theta}}\n",
    "    \\]\n",
    "</center>\n",
    "\n",
    "We have $\\sigma'(s) = \\sigma(s) ( 1 - \\sigma(s))$, so\n",
    "\n",
    "<center>\n",
    "    $\\frac{\\partial \\sigma}{\\partial \\theta} = \\hat{y}(1 - \\hat{y})X$\n",
    "</center>\n",
    "\n",
    "And we have\n",
    "\n",
    "<center>\n",
    "    \\[\n",
    "    \\frac{\\partial \\mathcal{L}}{\\partial {\\theta}}\n",
    "    = \\frac{\\hat{y} - y}{\\hat{y}(1 - \\hat{y})} \\cdot \\hat{y}(1 - \\hat{y})X\n",
    "    = (\\hat{y} - y)X \\\\\n",
    "    \\frac{\\partial \\mathcal{L}}{\\partial {\\theta}} = (\\hat{y} - y)X\n",
    "    \\]\n",
    "</center>\n",
    "\n",
    "For each sample from the dataset,\n",
    "\n",
    "<center>\n",
    "    \\[\n",
    "    \\frac{\\partial \\mathcal{L}(\\theta, x^i, y^i)}{\\partial {\\theta}} = (\\hat{y}^i - y^i)x^i\n",
    "    \\]\n",
    "</center>\n",
    "\n",
    "Using SGD, we have fomular to update parameters\n",
    "\n",
    "<center>\n",
    "    \\[\n",
    "    \\theta = \\theta + \\eta(\\hat{y}^i - y^i)x^i\n",
    "    \\]\n",
    "</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Softmax regression\n",
    "Using simple logistic regression, we only solve classification problem with only 1 class.\n",
    "\n",
    "To solve multiple classes classification problem by using logistic regression, we have to build multiple logistic regression models. This model is called ***one-vs-rest***.\n",
    "\n",
    "<img src=\"../images/softmax_regression_one_vs_rest.png\" width=\"600px\"/>\n",
    "\n",
    "$a_i$ with i = 1, 2, 3 ... C are almost independent and their sum can be larger or smaller than 1.\n",
    "\n",
    "<img src=\"../images/softmax_regression_softmax_net.png\" width=\"700px\"/>\n",
    "\n",
    "<img src=\"../images/softmax_regression_example.png\" width=\"700px\"/>\n",
    "\n",
    "Because we have to calculate $e^{z_i}$, if $z_i$ is large, $e^{z_i}$ will be very large and it causes value out of range error. We need to build a stable version of softmax.\n",
    "\n",
    "<center>\n",
    "    \\[\n",
    "    softmax(z^i)\n",
    "    = \\frac{e^{z_i}}{\\sum_j^C e^{z_j}}\n",
    "    = \\frac{e^{-max_j(z_j)} * e^{z_i}}{e^{-max_j(z_j)} * \\sum_j^C e^{z_j}}\n",
    "    = \\frac{e^{z_i - max_j(z_j)}}{\\sum_j^C e^{z_j - max_j(z_j)}}\n",
    "    \\]\n",
    "</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Loss function and Optimizer for Softmax regression\n",
    "Instead of having only 2 classes like logistic regression, softmax regression has C classes and it need another form of cross entropy loss function.\n",
    "\n",
    "Its name is ***CATEGORICAL CROSS ENTROPY***\n",
    "\n",
    "<center>\n",
    "    \\[\n",
    "    J(W; X, Y) = -\\sum_{i=1}^N \\sum_{j=1}^C y_{ji}\\log(a_{ji})\n",
    "    = -\\sum_{i=1}^N \\sum_{j=1}^C y_{ji}\\log\\frac{e^{{z}_i}}{\\sum_{k=1}^C e^{{z}_k}}\n",
    "    \\]\n",
    "</center>\n",
    "\n",
    "For each sample from the dataset,\n",
    "<center>\n",
    "    \\[\n",
    "    J({W}; {x}_i, {y}_i)\n",
    "    = -\\sum_{j=1}^C y_{ji}\\log\\frac{e^{{z}_ji}}{\\sum_{k=1}^C e^{{z}_ki}} \\\\\n",
    "    = -\\sum_{j=1}^C (y_{ji}{z}_ji - y_{ji}\\log{\\sum_{k=1}^C e^{{z}_ki}}) \\\\\n",
    "    = -\\sum_{j=1}^C y_{ji}{z}_ji + \\sum_{j=1}^C y_{ji}\\log{\\sum_{k=1}^C e^{{z}_ki}} \\\\\n",
    "    = -\\sum_{j=1}^C y_{ji}{z}_ji + \\log{\\sum_{k=1}^C e^{{z}_ki}}\n",
    "    \\]\n",
    "</center>\n",
    "\n",
    "Note:\n",
    "- $\\sum_{j=1}^C y_{ji}=1$ because it's the sum of probability\n",
    "- $\\log{\\sum_{k=1}^C e^{{z}_ki}}$ is independent with $j$ so we can remove $\\sum_{j=1}^C$\n",
    "\n",
    "To calculate derivative of $J$ with $W$, we can use the following fomular\n",
    "\n",
    "<center>\n",
    "    \\[\n",
    "    \\frac{\\partial J_i(W)}{\\partial W} = [\\frac{\\partial J_i(W)}{\\partial w_1}, \\frac{\\partial J_i(W)}{\\partial w_2}, \\dots, \\frac{\\partial J_i(W)}{\\partial w_C}]\n",
    "    \\]\n",
    "</center>\n",
    "\n",
    "and gradient of each column can be calculated by\n",
    "\n",
    "<center>\n",
    "    \\[\n",
    "    \\frac{\\partial J_i(W)}{\\partial w_j} \n",
    "    = -y_{ji}x_i + \\frac{e^{z_ji} x_i}{\\sum_{k=1}^C e^{z_ki}}\n",
    "    = -y_{ji}x_i + a_{ji} x_i\n",
    "    = x_i (a_{ji} - y_{ji})\n",
    "    \\]\n",
    "</center>\n",
    "\n",
    "Note:\n",
    "- In the first equation, because we do derivative with $w_j$, all elements in $\\sum_{k=1}^C e^{z_ki}$ are equal to 0 except $e^{z_ji}$\n",
    "- $e_{ji} = a_{ji} - y_{ji}$ is the different between the prediction and the real value\n",
    "\n",
    "Now, we have \n",
    "\n",
    "<center>\n",
    "    \\[\n",
    "    \\frac{\\partial J_i(W)}{\\partial W} = x_i[e_{1i}, e_{1i}, \\dots, e_{ji}] = x_i e_i\n",
    "    \\]\n",
    "</center>\n",
    "\n",
    "and for the whole dataset\n",
    "\n",
    "<center>\n",
    "    \\[\n",
    "    \\frac{\\partial J(W)}{\\partial W} = \\sum_{i=1}^N x_i e_{i} = XE\n",
    "    \\]\n",
    "</center>\n",
    "\n",
    "Using SGD, we have fomular to update parameters\n",
    "\n",
    "<center>\n",
    "    \\[\n",
    "    W = W + \\eta e_i x_i = W + \\eta(a_{i} - y_{i})x_i\n",
    "    \\]\n",
    "</center>\n",
    "\n",
    "**To conclude, Logistic Regression is a special case of Softmax Regression!!!**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Implementation example\n",
    "### 6.1. Implement from scratch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.2. Use `sklearn`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Homework"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  },
  "notify_time": "5",
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": false,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Table of Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
