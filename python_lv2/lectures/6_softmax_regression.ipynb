{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LESSON 6: SOFTMAX REGRESSION\n",
    "<table><tr>\n",
    "<td> <img src=\"../images/linear_logistic_regression_logo.jpeg\" width=\"600px\"/> </td>\n",
    "</tr></table>\n",
    "\n",
    "*This lecture was refered by [machinelearningcoban.com](https://machinelearningcoban.com/2017/02/17/softmax/)*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Softmax regression introduction\n",
    "Using simple logistic regression, we only solve classification problem with only 1 class.\n",
    "\n",
    "To solve multiple classes classification problem by using logistic regression, we have to build multiple logistic regression models. This model is called ***one-vs-rest***.\n",
    "\n",
    "<img src=\"../images/softmax_regression_one_vs_rest.png\" width=\"600px\"/>\n",
    "\n",
    "$a_i$ with i = 1, 2, 3 ... C are almost independent and their sum can be larger or smaller than 1.\n",
    "\n",
    "<img src=\"../images/softmax_regression_softmax_net.png\" width=\"700px\"/>\n",
    "\n",
    "<img src=\"../images/softmax_regression_example.png\" width=\"700px\"/>\n",
    "\n",
    "Because we have to calculate $e^{z_i}$, if $z_i$ is large, $e^{z_i}$ will be very large and it causes value out of range error. We need to build a stable version of softmax.\n",
    "\n",
    "<center>\n",
    "    \\[\n",
    "    softmax(z^i)\n",
    "    = \\frac{e^{z_i}}{\\sum_j^C e^{z_j}}\n",
    "    = \\frac{e^{-max_j(z_j)} * e^{z_i}}{e^{-max_j(z_j)} * \\sum_j^C e^{z_j}}\n",
    "    = \\frac{e^{z_i - max_j(z_j)}}{\\sum_j^C e^{z_j - max_j(z_j)}}\n",
    "    \\]\n",
    "</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Loss function and Optimizer for Softmax regression\n",
    "Instead of having only 2 classes like logistic regression, softmax regression has C classes and it need another form of cross entropy loss function.\n",
    "\n",
    "Its name is ***CATEGORICAL CROSS ENTROPY***\n",
    "\n",
    "<center>\n",
    "    \\[\n",
    "    J(W; X, Y) = -\\sum_{i=1}^N \\sum_{j=1}^C y_{ji}\\log(a_{ji})\n",
    "    = -\\sum_{i=1}^N \\sum_{j=1}^C y_{ji}\\log\\frac{e^{{z}_i}}{\\sum_{k=1}^C e^{{z}_k}}\n",
    "    \\]\n",
    "</center>\n",
    "\n",
    "For each sample from the dataset,\n",
    "<center>\n",
    "    \\[\n",
    "    J({W}; {x}_i, {y}_i)\n",
    "    = -\\sum_{j=1}^C y_{ji}\\log\\frac{e^{{z}_ji}}{\\sum_{k=1}^C e^{{z}_ki}} \\\\\n",
    "    = -\\sum_{j=1}^C (y_{ji}{z}_ji - y_{ji}\\log{\\sum_{k=1}^C e^{{z}_ki}}) \\\\\n",
    "    = -\\sum_{j=1}^C y_{ji}{z}_ji + \\sum_{j=1}^C y_{ji}\\log{\\sum_{k=1}^C e^{{z}_ki}} \\\\\n",
    "    = -\\sum_{j=1}^C y_{ji}{z}_ji + \\log{\\sum_{k=1}^C e^{{z}_ki}}\n",
    "    \\]\n",
    "</center>\n",
    "\n",
    "Note:\n",
    "- $\\sum_{j=1}^C y_{ji}=1$ because it's the sum of probability\n",
    "- $\\log{\\sum_{k=1}^C e^{{z}_ki}}$ is independent with $j$ so we can remove $\\sum_{j=1}^C$\n",
    "\n",
    "To calculate derivative of $J$ with $W$, we can use the following fomular\n",
    "\n",
    "<center>\n",
    "    \\[\n",
    "    \\frac{\\partial J_i(W)}{\\partial W} = [\\frac{\\partial J_i(W)}{\\partial w_1}, \\frac{\\partial J_i(W)}{\\partial w_2}, \\dots, \\frac{\\partial J_i(W)}{\\partial w_C}]\n",
    "    \\]\n",
    "</center>\n",
    "\n",
    "and gradient of each column can be calculated by\n",
    "\n",
    "<center>\n",
    "    \\[\n",
    "    \\frac{\\partial J_i(W)}{\\partial w_j} \n",
    "    = -y_{ji}x_i + \\frac{e^{z_ji} x_i}{\\sum_{k=1}^C e^{z_ki}}\n",
    "    = -y_{ji}x_i + a_{ji} x_i\n",
    "    = x_i (a_{ji} - y_{ji})\n",
    "    \\]\n",
    "</center>\n",
    "\n",
    "Note:\n",
    "- In the first equation, because we do derivative with $w_j$, all elements in $\\sum_{k=1}^C e^{z_ki}$ are equal to 0 except $e^{z_ji}$\n",
    "- $e_{ji} = a_{ji} - y_{ji}$ is the different between the prediction and the real value\n",
    "\n",
    "Now, we have \n",
    "\n",
    "<center>\n",
    "    \\[\n",
    "    \\frac{\\partial J_i(W)}{\\partial W} = x_i[e_{1i}, e_{1i}, \\dots, e_{ji}] = x_i e_i\n",
    "    \\]\n",
    "</center>\n",
    "\n",
    "and for the whole dataset\n",
    "\n",
    "<center>\n",
    "    \\[\n",
    "    \\frac{\\partial J(W)}{\\partial W} = \\sum_{i=1}^N x_i e_{i} = XE\n",
    "    \\]\n",
    "</center>\n",
    "\n",
    "Using SGD, we have fomular to update parameters\n",
    "\n",
    "<center>\n",
    "    \\[\n",
    "    W = W + \\eta e_i x_i = W + \\eta(a_{i} - y_{i})x_i\n",
    "    \\]\n",
    "</center>\n",
    "\n",
    "**To conclude, Logistic Regression is a special case of Softmax Regression!!!**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Implementation example"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1. Prepare library and data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2. Implement from scratch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3. Use `sklearn`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Homework"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  },
  "notify_time": "5",
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": false,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Table of Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
