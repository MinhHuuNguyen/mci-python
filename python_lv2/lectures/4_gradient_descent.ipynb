{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LESSON 4: GRADIENT DESCENT - OPTIMIZATION ALGORITHMS\n",
    "<table><tr>\n",
    "<td> <img src=\"../images/gd_logo.jpeg\" width=\"400px\"/> </td>\n",
    "</tr></table>\n",
    "\n",
    "*This lecture was refered by [machinelearningcoban.com](https://machinelearningcoban.com/2017/01/12/gradientdescent/)*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Gradient descent introduction\n",
    "In machine learning, we usually have to find the optimal point of a function (minimum or maximum).\n",
    "\n",
    "For example: finding minimum of MSE loss function of Linear regression ...\n",
    "\n",
    "In calculus, we split minimum point into 2 types: Local minimum and Global minimum\n",
    "\n",
    "<img src=\"../images/gd_local_vs_global.png\" width=\"350px\"/>\n",
    "\n",
    "In theory, we can solve derivative equation to find all of local minimum points and choose the smallest to find global minimum point.\n",
    "\n",
    "But, in reality, it's too hard (or maybe impossible) to solve the derivative equation because of the large-scale dataset, the complexity of the derivative equation or the high-dimension data points etc.\n",
    "\n",
    "That's why we need to build an algorithm to gradually archieve local minimum point instead of finding it directly.\n",
    "\n",
    "***GRADIENT DESCENT*** solved the problem and become the most important part in lots of machine learning algorithms and deep learning models till now."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Gradient descent for singlevariate function\n",
    "### 2.1. Formula\n",
    "We have a simple singlevariate function and $x^*$ is an minimum point.\n",
    "\n",
    "<img src=\"../images/gd_single_variate.png\" width=\"400px\"/>\n",
    "\n",
    "Assume that we have $x_t$ in the *t* iteration of gradient descent algorithm, our mission is to bring $x_t$ near to $x^*$.\n",
    "\n",
    "If $f'(x_t) > 0$, so $x_t > x^*$ and if $f'(x_t) < 0$, so $x_t < x^*$.\n",
    "\n",
    "We have $x_{t+1} = x_t + \\delta$ which delta has different sign of $f'(x_t)$ (if $f'(x_t) > 0$, so $\\delta < 0$ and vice versa).\n",
    "\n",
    "The larger distance between $x_t$ and $x^*$, the higher value of $|f'(x_t)|$. So,\n",
    "$\\delta = - \\eta f'(x_t)$ and $x_{t+1} = x_t - \\eta f'(x_t)$.\n",
    "\n",
    "$\\eta$ is call ***learning rate*** which is really important in optimization. And the sign - is the reason why we call this algorithm ***Gradient descent***."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2. Implementation\n",
    "We have an example:\n",
    "<center>\n",
    "    \\[\n",
    "    f(x) = x^2 + 5 \\sin(x) \\\\\n",
    "    f'(x) = 2x + 5 \\cos(x) \\\\\n",
    "    x_{t+1} = x_t - \\eta (2x + 5 \\cos(x))\n",
    "    \\]\n",
    "</center>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def my_function(x):\n",
    "    return x ** 2 + 5 * np.sin(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def my_grad(x):\n",
    "    return 2 * x + 5 * np.cos(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def my_gradient_descent(lr, x_0):\n",
    "    x = [x_0]\n",
    "    for it in range(100):\n",
    "        x_new = x[-1] - lr * my_grad(x[-1])\n",
    "        if abs(my_grad(x_new)) < 1e-3:\n",
    "            break\n",
    "        x.append(x_new)\n",
    "    return (x, it)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "([-5,\n",
       "  -4.141831092731613,\n",
       "  -3.0434140487394945,\n",
       "  -1.9371390635788721,\n",
       "  -1.370609623535342,\n",
       "  -1.1959138533062952,\n",
       "  -1.1398126662660861,\n",
       "  -1.1207324901805855,\n",
       "  -1.1140974995041208,\n",
       "  -1.1117718342401366,\n",
       "  -1.1109543623859697,\n",
       "  -1.1106667365268623],\n",
       " 11)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(solution_1, iteration_1) = my_gradient_descent(lr=.1, x_0=-5)\n",
    "(solution_1, iteration_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "([5,\n",
       "  3.8581689072683867,\n",
       "  3.463564567930569,\n",
       "  3.2451582916682646,\n",
       "  3.0934475688734215,\n",
       "  2.9741786797296776,\n",
       "  2.8723524342019475,\n",
       "  2.7798685851337033,\n",
       "  2.691538912182054,\n",
       "  2.6034429924417726,\n",
       "  2.512083663118539,\n",
       "  2.413825273788166,\n",
       "  2.3043909242955314,\n",
       "  2.178284700900974,\n",
       "  2.028031263811057,\n",
       "  1.8431593967550366,\n",
       "  1.6090315913519224,\n",
       "  1.3063382475764564,\n",
       "  0.9143774850440367,\n",
       "  0.42636006838025553,\n",
       "  -0.11415049832376267,\n",
       "  -0.5880663503407275,\n",
       "  -0.8864605464168875,\n",
       "  -1.025247680965368,\n",
       "  -1.0796417320111382,\n",
       "  -1.0995355411928176,\n",
       "  -1.1066334337506414,\n",
       "  -1.1091439570842945,\n",
       "  -1.1100292207856688,\n",
       "  -1.1103410483948122],\n",
       " 29)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(solution_2, iteration_2) = my_gradient_descent(lr=.1, x_0=5)\n",
    "(solution_2, iteration_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-3.2463941936103735"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "my_function(solution_1[-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-3.246394179661889"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "my_function(solution_2[-1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Compare initialization\n",
    "\n",
    "<table><tr>\n",
    "<td> <img src=\"../images/gd_compare_init_1.gif\" width=\"350px\"/> </td>\n",
    "<td> <img src=\"../images/gd_compare_init_2.gif\" width=\"350px\"/> </td>\n",
    "</tr></table>\n",
    "\n",
    "With initialization $x_0 = -5$, the solution is converged much faster than initialization $x_0 = 5$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Compare learning rate\n",
    "\n",
    "<table><tr>\n",
    "<td> <img src=\"../images/gd_compare_lr_1.gif\" width=\"350px\"/> </td>\n",
    "<td> <img src=\"../images/gd_compare_lr_2.gif\" width=\"350px\"/> </td>\n",
    "</tr></table>\n",
    "\n",
    "With learning rate $\\eta = 0.01$, the solution is converged too slow (almost 10 times compare with learning rate $\\eta = 0.1$)\n",
    "\n",
    "With learning rate $\\eta = 0.5$, the solution is converged faster but it cannot archieve the local minimum because of large step."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Gradient descent for multivariate function\n",
    "### 3.1. Formula\n",
    "One example of multivariate function is Linear regression.\n",
    "\n",
    "The loss function of Linear regression,\n",
    "\n",
    "<center>\n",
    "    \\[\n",
    "    MSE(w) = \\frac{1}{N}\\frac{1}{2}(Xw - y)^2\n",
    "    \\]\n",
    "</center>\n",
    "\n",
    "and the derivative of the loss function,\n",
    "\n",
    "<center>\n",
    "    \\[\n",
    "    \\frac{\\partial MSE}{\\partial w} = \\frac{1}{N}{X}^{T}\\cdot(Xw - y)\n",
    "    \\]\n",
    "</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2. Implementation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Prepare linear regression data and model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.linear_model import LinearRegression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('../data/linear_regression_salary_data.csv')\n",
    "\n",
    "X = np.array([df.YearsExperience.to_list()])\n",
    "y = np.array([df.Salary.to_list()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_X_ones(X):\n",
    "    x_1 = np.ones_like(X)\n",
    "    print('x_0.shape', x_1.shape)\n",
    "\n",
    "    X = np.concatenate((x_1, X), axis=0).T\n",
    "    print('X.shape', X.shape)\n",
    "\n",
    "    return X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x_0.shape (1, 30)\n",
      "X.shape (30, 2)\n"
     ]
    }
   ],
   "source": [
    "X_with_1 = prepare_X_ones(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[25792.20019867,  9449.96232146]])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sklearn_linear_regression = LinearRegression(fit_intercept=False)\n",
    "sklearn_linear_regression.fit(X_with_1, y.T)\n",
    "sklearn_linear_regression.coef_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Implement gradient descent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def my_mse(w, X, y):\n",
    "    N = X.shape[0]\n",
    "    return (1/ N) * (1 / 2) * np.linalg.norm(y - X.dot(w), 2) ** 2;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def my_mse_grad(w, X, y):\n",
    "    N = X.shape[0]\n",
    "    return 1 / N * X.T.dot(X.dot(w) - y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def my_mse_gradient_descent(lr, w_init, X, y):\n",
    "    w = [w_init]\n",
    "    for it in range(2000):\n",
    "        w_new = w[-1] - lr * my_mse_grad(w[-1], X, y)\n",
    "        if np.linalg.norm(my_mse_grad(w_new, X, y)) / len(w_new) < 1e-3:\n",
    "            break \n",
    "        w.append(w_new)\n",
    "    return (w, it) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "w_init = np.array([[2], [1]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "(solution_w_1, iteration_1) = my_mse_gradient_descent(0.05, w_init, X_with_1, y.T)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[25792.19080029],\n",
       "       [ 9449.96371613]])"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "solution_w_1[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1387"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "iteration_1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[25792.20019867,  9449.96232146]])"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sklearn_linear_regression.coef_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  },
  "notify_time": "5",
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": false,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Table of Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
