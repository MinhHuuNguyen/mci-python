{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LESSON 2: LINEAR REGRESSION\n",
    "<table><tr>\n",
    "<td> <img src=\"../images/linear_logistic_regression_logo.jpeg\" width=\"600px\"/> </td>\n",
    "</tr></table>\n",
    "\n",
    "*This lecture was refered by [machinelearningcoban.com](https://machinelearningcoban.com/2016/12/28/linearregression/)*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Linear regression introduction\n",
    "\n",
    "<img src=\"../images/ml_house_prices_example.png\" width=\"400px\"/>\n",
    "\n",
    "With an example of **House price prediction** problem, we have 3 features of a house:\n",
    "- ${x}_{1}$ is the size of the house (in ${m}^{2}$)\n",
    "- ${x}_{2}$ is the number of bedrooms in the house (in rooms)\n",
    "- ${x}_{3}$ is the distance from the house to the city center (in km)\n",
    "and the label price of the house $y$\n",
    "\n",
    "We have to build a function to calculate the price of the house from above features $\\vec{x}=[x_1, x_2, x_3]$.\n",
    "\n",
    "<center>\n",
    "    $\\hat{y} = \\theta_0 + \\theta_1 x_1 + \\theta_2 x_2 + \\theta_3 x_3$\n",
    "</center>\n",
    "\n",
    "- $\\vec{\\theta}=[\\theta_0, \\theta_1, \\theta_2, \\theta_3]^{T}$ is parameters of model\n",
    "- $\\hat{y}$ is a prediction of model and we expect that $\\hat{y}$ and $y$ are almost similar.\n",
    "\n",
    "We use a **linear** function and this is **regression** problem. That's why we call **LINEAR REGRESSION**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Loss function and Optimizer\n",
    "Generalize our problem to n features, we have a set of features $\\vec{x}=[x_1, x_2, ..., x_n]$.\n",
    "\n",
    "<center>\n",
    "    $\\hat{y}=\\theta_0+\\theta_1 x_1 + \\theta_2 x_2 +...+\\theta_n x_n$\n",
    "</center>\n",
    "\n",
    "Given $m$ training samples $X = \\vec{x}^{(1)}, \\vec{x}^{(2)},...,\\vec{x}^{(m)}$, linear regression finds $\\vec{\\theta}$ that minimizes the difference between $\\hat{y}$ and $y$.\n",
    "\n",
    "To minimize the difference, we have to build a **LOSS FUNCTION** and for linear regression, we use Mean Square Error (MSE).\n",
    "\n",
    "<center>\n",
    "    \\[\n",
    "    MSE(\\vec{\\theta})\n",
    "    = \\frac{1}{2}\\sum_{i=1}^{m}(\\hat{y}^{(i)} - y^{(i)})^2 \\\\\n",
    "    = \\frac{1}{2}\\sum_{i=1}^{m}(\\vec{x}^{(i)}\\vec{\\theta} - y^{(i)})^2 \\\\\n",
    "    = \\frac{1}{2}(X\\vec{\\theta} - y)^2 \\\\\n",
    "    \\]\n",
    "</center>\n",
    "\n",
    "We need to find the $\\vec{\\theta}$ to minimize the value of MSE function and this $\\vec{\\theta}$ called an ***optimal point***.\n",
    "\n",
    "<center>\n",
    "    $\\vec{\\theta}^{*} = \\arg\\min_{\\theta} \\mathcal{L}(\\theta)$\n",
    "</center>\n",
    "\n",
    "To find the optimal point $\\vec{\\theta}^{*}$, we solve the equation:\n",
    "\n",
    "<center>\n",
    "    \\[\n",
    "    \\frac{\\partial MSE}{\\partial\\vec{\\theta}}\n",
    "    = \\frac{1}{2}\\cdot2\\cdot(X\\vec{\\theta} - y)\\cdot{X}^{T} \\\\\n",
    "    = {X}^{T}\\cdot(X\\vec{\\theta} - y) = 0\n",
    "    \\]\n",
    "</center>\n",
    "\n",
    "We can have the above equation because we have:\n",
    "<center>\n",
    "    \\[\n",
    "    \\frac{\\partial Ax+b}{\\partial x} = {A}^{T}\n",
    "    \\]\n",
    "</center>\n",
    "\n",
    "and $(X\\vec{\\theta} - y)$ is a scalar so we can use the commutative principle.\n",
    "\n",
    "Back to the equation\n",
    "<center>\n",
    "    \\[\n",
    "    {X}^{T}\\cdot(X\\vec{\\theta} - y) = 0 \\\\\n",
    "    {X}^{T}X\\vec{\\theta} = {X}^{T}y \\\\\n",
    "    \\vec{\\theta} = {({X}^{T}X})^{-1}{X}^{T}y\n",
    "    \\]\n",
    "</center>\n",
    "\n",
    "Finally, we have $\\vec{\\theta}^{*} = {({X}^{T}X})^{-1}{X}^{T}y$ is the solution of $\\frac{\\partial MSE}{\\partial\\vec{\\theta}} = 0$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Implementation example\n",
    "### 3.1. Implement from scratch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2. Use `sklearn`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Homework\n",
    "### 4.1. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  },
  "notify_time": "5",
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": false,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Table of Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
